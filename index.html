<!doctype html>
<html lang="en">

	<head>
		<meta charset="utf-8">

		<title>xfail and skip: What to do with tests you know will fail</title>

		
		
		<meta name="apple-mobile-web-app-capable" content="yes" />
		<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<link rel="stylesheet" href="reveal.js/dist/reveal.css">
		<link rel="stylesheet" href="reveal.js/css/theme/white.css" id="theme">

	
		<link rel="stylesheet" href="css/custom.css">
	


		<!-- For syntax highlighting -->
		
		<link rel="stylesheet" href="css/xcode-pg.css">

		<!-- If the query includes 'print-pdf', use the PDF print sheet -->
		<script>
			document.write( '<link rel="stylesheet" href="reveal.js/css/print/' + ( window.location.search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );
		</script>

		<!--[if lt IE 9]>
		<script src="reveal.js/lib/js/html5shiv.js"></script>
		<![endif]-->
	</head>

	<body>

		<div class="reveal">

			<div class="slides">

				
				<section data-markdown data-separator="^\n---\n$" data-separator-vertical="^\n--\n$" data-notes="^Note:">
					<textarea data-template>
<h1 style="font-size: 3em"><tt>xfail</tt> and <tt>skip</tt></h1>
<h2 style="font-size: 2.5em">What to do with tests you know will fail</h2>
<br/>
<br/>
<br/>
<span style="font-size: 2.25em">
Paul Ganssle
</span>
<br/>
<br/>
<img src="images/pganssle-logos.svg" height="40px" alt="@pganssle">
<br/>
<br/>
<span style="font-size: 1em;"><em>This talk on Github:
<a href="https://github.com/pganssle-talks/pytexas-2022-xfail">pganssle-talks/pytexas-2022-xfail</a></em>
</span>
<br/>
<a rel="license" href="https://creativecommons.org/publicdomain/zero/1.0/">
    <img src="external-images/logos/cc-zero.svg" height="45px">
</a>
<br/>

Notes:

					</textarea>
				</section>
				
				<section data-markdown data-separator="^\n---\n$" data-separator-vertical="^\n--\n$" data-notes="^Note:">
					<textarea data-template>
```python
import math

def is_perfect_square(n: int) -> bool:
    """Determine if any int i exists such that i Ã— i = n."""
    s = math.sqrt(n)
    return s == int(s)
```

With tests:

```python
import square_mod

import pytest


@pytest.mark.parametrize("n", [0, 1, 2, 4, 9, 16, 25, 36])
def test_squares(n):
    assert square_mod.is_perfect_square(n)


@pytest.mark.parametrize("n", [3, 5, 6, 7, 8, 27, 32])
def test_non_squares(n):
    assert not square_mod.is_perfect_square(n)
```
<br/>

<pre>
<tt class="hljs">
<b>================================================ test session starts ================================================</b>
platform linux -- Python 3.10.0, pytest-7.1.1, pluggy-1.0.0
<b>collected 14 items                                                                                                  </b>

test_square_mod.py <font color="green">..............                                                                             [100%]</font>

<font color="green">================================================ </font><font color="green"><b>14 passed</b></font><font color="green"> in 0.02s =================================================</font>
</tt></pre>

Notes:

So let's imagine that I have this function that determines if a given integer is a perfect square, and some tests for that function, and we'll add some tests for it that show that yes indeed that's what this function does - it gives True for all these perfect squares and False for all these non-square numbers. Perfect work for a Thursday afternoon, let's go ahead and ship it straight to production, no problem!


--

## Found a bug: Add a test

```python
def test_negative():
    assert not square_mod.is_perfect_square(-4)
```
<br/><br/>

<pre>
<tt class="hljs">
<b>================================================ test session starts ================================================</b>
platform linux -- Python 3.10.0, pytest-7.1.1, pluggy-1.0.0
<b>collected 15 items                                                                                                  </b>

test_square_mod.py <font color="green">..............</font><font color="#8B0000">F                                                                            [100%]</font>

===================================================== FAILURES ======================================================
<font color="#8B0000"><b>___________________________________________________ test_negative ___________________________________________________</b></font>

    <font color="#729FCF">def</font> <font color="green">test_negative</font>():
&gt;       <font color="#729FCF">assert</font> <font color="#AD7FA8">not</font> square_mod.is_perfect_square(-<font color="#729FCF">4</font>)

<font color="#8B0000"><b>test_square_mod.py</b></font>:17:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

n = -4

    <font color="#729FCF">def</font> <font color="green">is_perfect_square</font>(n: <font color="#34E2E2">int</font>) -&gt; <font color="#34E2E2">bool</font>:
        <font color="green">&quot;&quot;&quot;Determine if any int i exists such that i Ã— i = n.&quot;&quot;&quot;</font>
&gt;       s = math.sqrt(n)
<font color="#8B0000"><b>E       ValueError: math domain error</b></font>

<font color="#8B0000"><b>square_mod.py</b></font>:5: ValueError
============================================== short test summary info ==============================================
FAILED test_square_mod.py::test_negative - ValueError: math domain error
<font color="#8B0000">=========================================== </font><font color="#8B0000"><b>1 failed</b></font>, <font color="green">14 passed</font><font color="#8B0000"> in 0.08s ============================================</font>
</tt>
</pre>

--

## xfail: Tests that are *expected* to fail


```python
@pytest.mark.xfail(reason="Bug #11493: Negative values not supported!")
@pytest.mark.parametrize("n", [-1, -3, -4])
def test_negative(n):
    # When called with a negative value for n, this test raises ValueError!
    assert not square_mod.is_perfect_square(m)
```

<br/><br/>
Failure is expected, so the tests pass:

<pre><tt class="hljs">
<b>================================================ test session starts ================================================</b>
platform linux -- Python 3.10.0, pytest-7.1.1, pluggy-1.0.0
<b>collected 17 items                                                                                                  </b>

test_square_mod.py <font color="#10BA13">..............</font><font color="#C4A000">xxx</font><font color="#10BA13">                                                                          [100%]</font>

<font color="#10BA13">=========================================== </font><font color="#4BE234"><b>14 passed</b></font>, <font color="#C4A000">3 xfailed</font><font color="#10BA13"> in 0.03s ===========================================</font>
</tt></pre>

--

## Being stricter about *why* the test is failing

```python
@pytest.mark.xfail(
    raises=ValueError,
    reason="Bug #11493: Negative values not supported!"
    )
@pytest.mark.parametrize("n", [-1, -3, -4])
def test_negative(n):
    # When called with a negative value for n, this test raises ValueError!
    assert not square_mod.is_perfect_square(m)
```

<br/>

<pre><tt class="hljs">
<b>================================================ test session starts ================================================</b>
platform linux -- Python 3.10.0, pytest-7.1.1, pluggy-1.0.0
<b>collected 17 items                                                                                                  </b>

test_square_mod.py <font color="#10BA13">..............</font><font color="#F61010">FFF                                                                          [100%]</font>

===================================================== FAILURES ======================================================
<font color="#EF2929"><b>_________________________________________________ test_negative[-1] _________________________________________________</b></font>
<font color="#EF2929"><b>test_square_mod.py</b></font>:21: in test_negative
    <font color="#729FCF">assert</font> <font color="#AD7FA8">not</font> square_mod.is_perfect_square(m)
<font color="#EF2929"><b>E   NameError: name &apos;m&apos; is not defined</b></font>
<font color="#EF2929"><b>_________________________________________________ test_negative[-3] _________________________________________________</b></font>
<font color="#EF2929"><b>test_square_mod.py</b></font>:21: in test_negative
    <font color="#729FCF">assert</font> <font color="#AD7FA8">not</font> square_mod.is_perfect_square(m)
<font color="#EF2929"><b>E   NameError: name &apos;m&apos; is not defined</b></font>
<font color="#EF2929"><b>_________________________________________________ test_negative[-4] _________________________________________________</b></font>
<font color="#EF2929"><b>test_square_mod.py</b></font>:21: in test_negative
    <font color="#729FCF">assert</font> <font color="#AD7FA8">not</font> square_mod.is_perfect_square(m)
<font color="#EF2929"><b>E   NameError: name &apos;m&apos; is not defined</b></font>
============================================== short test summary info ==============================================
FAILED test_square_mod.py::test_negative[-1] - NameError: name &apos;m&apos; is not defined
FAILED test_square_mod.py::test_negative[-3] - NameError: name &apos;m&apos; is not defined
FAILED test_square_mod.py::test_negative[-4] - NameError: name &apos;m&apos; is not defined
<font color="#F61010">=========================================== </font><font color="#EF2929"><b>3 failed</b></font>, <font color="#10BA13">14 passed</font><font color="#F61010"> in 0.09s ============================================</font>
</tt></pre>

					</textarea>
				</section>
				
				<section data-markdown data-separator="^\n---\n$" data-separator-vertical="^\n--\n$" data-notes="^Note:">
					<textarea data-template>
# `skipif` vs `xfail`

<br/>

## `skipif` â€” Use for tests that *are supposed to fail*.

```python
@pytest.mark.skipif(sys.version_info < (3, 8),
                    reason="Module-level __getattr__ introduced in Python 3.8")
def test_module_getattr_works():
    with clear_dateutil_imports():
        import dateutil
        assert dateutil.tz is not None  # Lazy-loaded
```

<br/>

## `xfail` â€” Use for tests that *currently fail, but shouldn't*.

```python
@pytest.mark.xfail(reason="Fractional hours and minutes not implemented yet!")
def test_fractional_hour():
    # ISO 8601 allows fractional hours and minutes
    assert (dateutil.parser.isoparse("2021-03-26T14.5") ==
            datetime.datetime(2021, 3, 26, 14, 30))
```

					</textarea>
				</section>
				
				<section data-markdown data-separator="^\n---\n$" data-separator-vertical="^\n--\n$" data-notes="^Note:">
					<textarea data-template>
# `XFAIL` becomes `XPASS`

```python
def is_perfect_square(n: int) -> bool:
    """Determine if any real integer i exists such that i Ã— i = n."""

    # Negative numbers are not squares according to the definition of the function
    if n < 0:
        return False

    s = math.sqrt(n)
    return s == int(s)
```
<br/><br/>

<pre><tt class="hljs">
<b>================================================ test session starts ================================================</b>
platform linux -- Python 3.10.0, pytest-7.1.1, pluggy-1.0.0
<b>collected 15 items                                                                                                  </b>

test_square_mod.py <font color="#10BA13">..............</font><font color="#C4A000">X                                                                            [100%]</font>

<font color="#C4A000">=========================================== </font><font color="#10BA13">14 passed</font>, <font color="#FCE94F"><b>1 xpassed</b></font><font color="#C4A000"> in 0.02s ===========================================</font>
</tt></pre>

--

# Treating failure to fail as a failure
<br/>

```python
@pytest.mark.xfail(strict=True,
                   raises=ValueError,
                   reason="Bug #11493: Negative values not supported")
@pytest.mark.parametrize("n", [-1, -3, -4])
def test_negative(n):
    assert not square_mod.is_perfect_square(n)
```
<br/>

<pre><tt class="hljs">
<b>================================================ test session starts ================================================</b>
platform linux -- Python 3.10.0, pytest-7.1.1, pluggy-1.0.0
<b>collected 17 items</b>

test_square_mod.py <font color="#10BA13">..............</font><font color="#F61010">FFF                                                                          [100%]</font>

===================================================== FAILURES ======================================================
<font color="#EF2929"><b>_________________________________________________ test_negative[-1] _________________________________________________</b></font>
[XPASS(strict)] Bug #11493: Negative values not supported
<font color="#EF2929"><b>_________________________________________________ test_negative[-3] _________________________________________________</b></font>
[XPASS(strict)] Bug #11493: Negative values not supported
<font color="#EF2929"><b>_________________________________________________ test_negative[-4] _________________________________________________</b></font>
[XPASS(strict)] Bug #11493: Negative values not supported
============================================== short test summary info ==============================================
FAILED test_square_mod.py::test_negative[-1]
FAILED test_square_mod.py::test_negative[-3]
FAILED test_square_mod.py::test_negative[-4]
<font color="#F61010">=========================================== </font><font color="#EF2929"><b>3 failed</b></font>, <font color="#10BA13">14 passed</font><font color="#F61010"> in 0.03s ============================================</font>
</tt></pre>

--

## Make `strict=True` the default:

<br/>

In `tox.ini` / `pytest.ini`:

```ini
[pytest]
xfail_strict = True
```

<br/>

In `setup.cfg`:

```ini
[tool:pytest]
xfail_strict = True
```

<br/>

In `pyproject.toml`:

```toml
[tool.pytest.ini_options]
xfail_struct = True
```

<br/>

See [the pytest documentation](https://docs.pytest.org/en/latest/reference/customize.html) for the latest options for global configuration, or [the documentation on the `xfail_strict` option](https://docs.pytest.org/en/latest/reference/reference.html#confval-xfail_strict).

					</textarea>
				</section>
				
				<section data-markdown data-separator="^\n---\n$" data-separator-vertical="^\n--\n$" data-notes="^Note:">
					<textarea data-template>
# Why should I care?
<br/>

<div>
<h2>Document your acceptance criteria</h2>

Adding a failing test to the test suite documents the conditions necessary to fix the bug.
</div>
<br/>
<div class="fragment">
<h2>Test your tests!</h2>
Start by writing the xfailing test, if it doesn't fail, your test won't catch the regression.
</div>
<br/>
<div class="fragment">
<h2>Impose regression tests early!</h2>

Your test suite will fail if you accidentally fix a bug - remove the `xfail` and you'll prevent regressions from the moment you merged!
</div>

					</textarea>
				</section>
				
				<section data-markdown data-separator="^\n---\n$" data-separator-vertical="^\n--\n$" data-notes="^Note:">
					<textarea data-template>
# Using `xfail` and `skipif`: Markers

Bare decorator (with or without `reason`)

```python
@pytest.mark.xfail
def test_my_failing_function():
    assert my_function(-3) == 2


@pytest.mark.xfail(reason="NaN handling is not working properly yet.")
def test_float_function():
    float_function(float("nan"))


@pytest.mark.skip(reason="Probably shouldn't do unconditional skipping")
def test_this_is_pointless():
    some_random_function()
```

--

# Using `xfail` and `skipif`: Markers

## Boolean condition

```python
@pytest.mark.xfail(sys.version_info > (3, 10),
                   reason="AST handling changed in Python 3.10")
def test_some_ast_tomfoolery():
    my_ast_tomfoolery_function("path/to/python_file.py")


@pytest.mark.skipif(sys.version_info < (3, 9),
                    reason="zoneinfo introduced in Python 3.9")
def test_zoneinfo():
    import zoneinfo
    tz = zoneinfo.ZoneInfo("America/Chicago")

    ...
```

<br/>

## Condition string

<br/>

```python
@pytest.mark.skipif("not hasattr(os, 'fspath')")
def path_normalization():
    assert os.fspath(MyPathClass("a/b/c")) == "a/b/c"
```

*Condition strings are discouraged*

Notes:

When using boolean conditions, the `reason` parameter must be specified. `pytest` also supports specifying skip and xfail conditions with a string, which has some weird scoping rules that you can look up. This is the older way of specifying conditions and it is now discouraged, but the one advantage it has over boolean conditions is that the condition itself can serve as the reported reason for the skip, so it is not required to specify `reason`.

--

# Using `xfail` and `skipif`: Markers

## The `run` parameter

```python
@pytest.mark.xfail(
    run=False
    reason="Test will segfault if run!"
)
def test_surrogate_characters():
    parsed = datetime.fromisoformat("2022-03-26\ud800)13:15:04")
    expected = datetime(2022, 3, 26, 13, 15, 4)
    assert parsed == expected
```

<br/>

```python
@pytest.mark.xfail(
    sys.version_info < (3, 9),
    run=False,
    reason="Test segfaults earlier than Python 3.9"
)
def test_conditional_segfault():
    function_that_segfaults_on_38()
```

Notes:

Using `run=False` basically turns an `xfail` into a `skip`, but semantically it may be useful for you to mark that a given test should succeed but doesn't. `run=False` only applies when the test is actually marked as `xfail`, so if you specify a condition, this basically turns the test into `skipif`.

--

# Using `xfail` and `skipif`: Parameterized tests

<br/>

```python
@pytest.mark.parametrize("n", [
    1,
    pytest.param(0,
        marks=pytest.mark.xfail(
            raises=ZeroDivisionError,
            reason="Zero not handled correctly"
        )
    ),
    pytest.param(-1,
        marks=pytest.mark.xfail(
        reason="Not working for negative numbers."
        )
    ),
    5,
])
def test_is_euler_number(n):
    assert is_euler_number(n)
```

<br/>
<br/>

<pre>
<tt class="hljs">
<b>================================================ test session starts =================================================</b>
platform linux -- Python 3.10.2, pytest-7.1.1, pluggy-1.0.0
<b>collected 4 items                                                                                                    </b>

test_euler_numbers.py <font color="#10BA13">.</font><font color="#C4A000">xx</font><font color="#10BA13">.                                                                                     [100%]</font>

<font color="#10BA13">============================================ </font><font color="#4BE234"><b>2 passed</b></font>, <font color="#C4A000">2 xfailed</font><font color="#10BA13"> in 0.03s ============================================</font>
</tt>
</pre>

					</textarea>
				</section>
				
				<section data-markdown data-separator="^\n---\n$" data-separator-vertical="^\n--\n$" data-notes="^Note:">
					<textarea data-template>
# Using `xfail` and `skip`

```python
@pytest.mark.parametrize("payload", list(
    pathlib.Path.glob("test_data/test_data_*")
))
def test_process_data_files(payload):
    with open(payload, "rb") as f:
        test_data = load_data(f)

    if test_data.num_elements > 255:
        # Don't actually use this! See next slide!
        pytest.xfail(reason="bug #2445: Processing of data files with > 255 fails.")
```

<br/>

Can embed these into functions, context managers and fixtures

<br/>

```python
@contextlib.contextmanager
def in_local_timezone(tz: str) -> Iterator[None]:
    if sys.platform.startswith("win") or os.eviron.get("TZ_CHANGE_DISALLOWED") == "true":
        pytest.skip("Time zone change not allowed")

    old_tz = get_current_tz()
    set_tz(tz)
    time.tzset()
    yield
    set_tz(old_tz)
    time.tzset()


def test_in_new_york():
    # This test is skipped automatically in platforms and environments that
    # don't support changing the time zone.
    with in_local_timezone("America/New_York"):
        assert my_date_function("2021-01-01") > my_date_function("2020-01-01")
```

--

# Papercut: `pytest.xfail()` is basically skip!

- `pytest.xfail()` stops execution immediately â€” there is no way to get `XPASS`
- `xfail_strict` has no effect
- [This is deliberate](https://github.com/pytest-dev/pytest/issues/7071)

<br/><br/>

# Solution: [`pytest-runtime-xfail`](https://github.com/okken/pytest-runtime-xfail)

```python
@pytest.mark.parametrize("payload", list(
    pathlib.Path.glob("test_data/test_data_*")
))
def test_process_data_files(payload, runtime_xfail):
    with open(payload, "rb") as f:
        test_data = load_data(f)

    if test_data.num_elements > 255:
        # Don't actually use this! See next slide!
        runtime_xfail(reason="bug #2445: Processing of data files with > 255 fails.")
```

--

# Papercut: `xfail` with `hypothesis`

```python
import hypothesis
from hypothesis import strategies as st

@hypothesis.given(n=st.integers())
def test_my_function(n: int) -> None:
    if n < 0:
        pytest.xfail("Zero and negative numbers not working!")

    assert my_function(n) > n
```
<br/>

## Problems:

- Reports a single top-level `XFAIL`:

<pre>
<tt class="hljs">
<b>================================================ test session starts =================================================</b>
platform linux -- Python 3.10.2, pytest-7.1.1, pluggy-1.0.0
plugins: hypothesis-6.39.4
<b>collected 1 item                                                                                                     </b>

test_hypothesis.py <font color="#C4A000">x                                                                                           [100%]</font>

<font color="#C4A000">================================================= </font><font color="#FCE94F"><b>1 xfailed</b></font><font color="#C4A000"> in 0.22s =================================================</font>
</tt>
</pre>

- Disguises real failures in non-`xfail` cases
- Doesn't work with `pytest-runtime-xfail`

					</textarea>
				</section>
				
				<section data-markdown data-separator="^\n---\n$" data-separator-vertical="^\n--\n$" data-notes="^Note:">
					<textarea data-template>
# `xfail` and code coverage

```python
def do_fizzbuzz(n: int) -> str | int:
    if (n % 15 == 0):
        return "fizzbuzz"

    if (n % 3 == 0):
        return "fizz"

    if (n % 5 == 0):
        return "buzz"

    return n
```

Some passing tests:

```python
import pytest

@pytest.mark.parametrize("n, expected", [
    (3, "fizz"),
    (1, 1),
    (2, 2),
    (14, 14),
    (15, "fizzbuzz"),
])
def test_fizzbuzz(n, expected):
    assert do_fizzbuzz(n) == expected
```

One failing test:

```python
@pytest.mark.xfail(reason="Not implemented yet")
def test_fizzbuzz_failure():
    """Fizzbuzz should fail when passed a negative number."""
    with pytest.raises(ValueError):
        do_fizzbuzz(-5)  # Foreshadowing: this hits the n == 5 case!
```

--

Run with `pytest --cov`:

<pre>
<tt class="hljs">
<b>================================================ test session starts =================================================</b>
platform linux -- Python 3.10.2, pytest-7.1.1, pluggy-1.0.0
plugins: cov-3.0.0
<b>collected 23 items / 17 deselected / 6 selected</b>

test_fizzbuzz.py <font color="#10BA13">.....</font><font color="#C4A000">x</font><font color="#10BA13">                                                                                        [100%]</font>

---------- coverage: platform linux, python 3.10.2-final-0 -----------
Name          Stmts   Miss  Cover
---------------------------------
fizzbuzz.py       8      0   100%
---------------------------------
TOTAL             8      0   100%


<font color="#10BA13">==================================== </font><font color="#4BE234"><b>5 passed</b></font>, <font color="#C4A000">17 deselected</font>, <font color="#C4A000">1 xfailed</font><font color="#10BA13"> in 0.05s =====================================</font>
</tt>
</pre>

âœ¨ðŸŽ‰ 100% code coverage! ðŸŽ‰âœ¨

--

```python
def do_fizzbuzz(n: int) -> str | int:
    if n < 0:
        raise ValueError(f"Number must be positive, but got {n}")

    if (n % 15 == 0):
        return "fizzbuzz"

    if (n % 3 == 0):
        return "fizz"

    if (n % 5 == 0):
        return "buzz"  # This line is no longer covered!

    return n
```

<br/>

Remove the `xfail` decorator, code coverage goes **down**! ðŸ˜°ðŸ˜°ðŸ˜°

<pre>
<tt class="hljs">
<b>================================================ test session starts =================================================</b>
platform linux -- Python 3.10.2, pytest-7.1.1, pluggy-1.0.0
plugins: cov-3.0.0
<b>collected 23 items / 17 deselected / 6 selected                                                                      </b>

test_fizzbuzz.py <font color="#10BA13">.....</font><font color="#C4A000">X                                                                                        [100%]</font>

---------- coverage: platform linux, python 3.10.2-final-0 -----------
Name          Stmts   Miss  Cover
---------------------------------
fizzbuzz.py      10      1    90%
---------------------------------
TOTAL            10      1    90%


<font color="#C4A000">==================================== </font><font color="#10BA13">5 passed</font>, <font color="#FCE94F"><b>17 deselected</b></font>, <font color="#FCE94F"><b>1 xpassed</b></font><font color="#C4A000"> in 0.05s =====================================</font>
</tt>
</pre>

--

```python
def do_fizzbuzz(n: int) -> str | int:
    if (n % 15 == 0):
        return "fizzbuzz"

    if (n % 3 == 0):
        return "fizz"

    if (n % 5 == 0):
        return "buzz"

    return n
```

Some passing tests:

```python
import pytest

@pytest.mark.parametrize("n, expected", [
    (3, "fizz"),
    (1, 1),
    (2, 2),
    # 3, 1, 2, but no 5!
    (14, 14),
    (15, "fizzbuzz"),
])
def test_fizzbuzz(n, expected):
    assert do_fizzbuzz(n) == expected
```

One failing test:

```python
@pytest.mark.xfail(reason="Not implemented yet")
def test_fizzbuzz_failure():
    """Fizzbuzz should fail when passed a negative number."""
    with pytest.raises(ValueError):
        do_fizzbuzz(-5)  # -5 % 5 == 0
```

--

## The Problem

- Code coverage metrics assume that your tests *passed*

- Expected failures don't pass, but they also don't cause the test suite to fail!

Notes:

Some lines are hit by well-executed passing tests, and some lines are hit as collateral damage from the flailing death throes of an expected failure. They both show up the same in the coverage statistics!


--

# The Solution: Exclude expected failures from coverage

<br/>

With `pytest-cov`, attach the `no_cover` marker automatically in your `conftest.py`:

<br/>

```python
def pytest_collection_modifyitems(items):
    for item in items:
        marker = item.get_closest_marker("xfail")

        # Need to query the args because conditional xfail tests still have
        # the xfail mark even if they are not expected to fail
        if marker and (not marker.args or marker.args[0]):
            item.add_marker(pytest.mark.no_cover)
```

<br/>

See https://stackoverflow.com/q/53191930/467366

<br/>

[**`coverage.py` wants to add this feature natively**](https://github.com/nedbat/coveragepy/issues/727) â€” you could be the one to implement it!

--

# Does this matter?

<br/>

**Probably not** <!-- .element: class="fragment" -->

					</textarea>
				</section>
				
				<section data-markdown data-separator="^\n---\n$" data-separator-vertical="^\n--\n$" data-notes="^Note:">
					<textarea data-template>
# Using `unittest`

```python
import unittest

class MyTest(unittest.TestCase):
    def test_pass(self):
        self.assertTrue(True)

    @unittest.expectedFailure
    def test_xfail(self):
        self.assertTrue(False)

    @unittest.expectedFailure
    def test_xpass(self):
        self.assertTrue(True)

```

<br/>

<div class="fragment disappearing-fragment nospace-fragment fade-out" data-fragment-index="0">
Run with <tt>unittest</tt>:

<pre>
<tt class="hljs">
$ python -m unittest -v
test_pass (test_unittest.MyTest) ... ok
test_xfail (test_unittest.MyTest) ... expected failure
test_xpass (test_unittest.MyTest) ... unexpected success

----------------------------------------------------------------------
Ran 3 tests in 0.001s

FAILED (expected failures=1, unexpected successes=1)
</tt></pre>
</div>

<div class="fragment nospace-fragment fade-in" data-fragment-index="0">
Run with <tt>pytest</tt>:

<pre>
<tt class="hljs">
$ pytest test_unittest.py 
<b>================================================ test session starts =================================================</b>
platform linux -- Python 3.10.2, pytest-7.1.1, pluggy-1.0.0
<b>collected 3 items                                                                                                    </b>

test_unittest.py <font color="#10BA13">.</font><font color="#C4A000">x</font><font color="#F61010">F                                                                                           [100%]</font>

====================================================== FAILURES ======================================================
<font color="#EF2929"><b>_________________________________________________ MyTest.test_xpass __________________________________________________</b></font>
Unexpected success
============================================== short test summary info ===============================================
FAILED test_unittest.py::MyTest::test_xpass
<font color="#F61010">======================================= </font><font color="#EF2929"><b>1 failed</b></font>, <font color="#10BA13">1 passed</font>, <font color="#C4A000">1 xfailed</font><font color="#F61010"> in 0.04s =======================================</font>
</tt>
</pre>
</div>

--

# `unittest`: Missing features

- No `strict=False` or `run=False`
- No support for `raises`
- No way to specify a reason
- Conditional `xfail` not built-in:

  <br/>

```python
def conditional_xfail(condition):
    if condition:
        return unittest.expectedFailure
    else:
        return lambda x: x
```

--

# Skipping with `unittest`
<br/>

Decorators:

```python
class MySkipTest(unittest.TestCase):
    @unittest.skip("Unconditional skipping")
    def skip_unconditional(self):
        pass

    @unittest.skipIf(sys.platform.startswith("win"), "Not supported on Windows")
    def test_nix_only(self):
        ...

    @unittest.skipUnless(sys.platform.startswith("win"), "Only supported on Windows")
    def test_windows_only(self):
        ...
```

<br/>
<br/>

At runtime:

```python
    def test_three_day_weekend(self):
        if 4 <= datetime.now().weekday() <= 6:
            self.skipTest("This test has negotiated a 3-day weekend.")

        ...



    def test_api_integration(self):
        r = requests.get("https://url.to/some/resource")
        if r.status_code != 200:
            raise unittest.SkipTest("Resource not available")

        ...
```

					</textarea>
				</section>
				
				<section data-markdown data-separator="^\n---\n$" data-separator-vertical="^\n--\n$" data-notes="^Note:">
					<textarea data-template>
# Bonus for pedants: A pseudo-TDD workflow

<br/>

## Test driven development:

1. Write a minimal failing test.
2. Write just enough code to make the test pass.
3. Goto 1.

<br/><br/>

## Reality:

1. Make some quick sketch to see if your idea even works.
2. Maybe write some tests for the sketch.
3. Realize your approach has some fundamental flaws, rip out half of the code and break a bunch of the tests.
4. Clean up and refactor tests.

--

# Bonus for pedants: A pseudo-TDD workflow

```python
def is_perfect_square(n: int) -> bool:
    """Determine if any int i exists such that i Ã— i = n."""
    s = math.sqrt(n)
    return s == int(s)
```

<br/>

Simple enough fix â€” add the fix and the test at the same time:

```python
def is_perfect_square(n: int) -> bool:
    """Determine if any int i exists such that i Ã— i = n."""
    if n < 0:
        return False  # No negative numbers are perfect squares

    s = math.sqrt(n)
    return s == int(s)


def test_negative():
    """No negative values are perfect squares."""
    assert not is_perfect_square(-4)
```

https://blog.ganssle.io/articles/2021/11/pseudo-tdd-xfail.html

--

# Rewriting history

```python
@pytest.mark.xfail(raises=ValueError, strict=True,
                   reason="Domain error when passed negative numbers.")
def test_negative():
    ...
```

<br/>

Use `git add -p` or `hg commit -i` to selectively commit the test *before* the fix:

```diff
@@ -7,6 +7,11 @@ def is_perfect_square(n: int) -> bool:
     s = math.sqrt(n)
     return s == int(s)

+@pytest.mark.xfail(raises=ValueError, strict=True,
+                   reason="Domain error")
+def test_negative():
+    """No negative numbers are perfect squares."""
+    assert not is_perfect_square(-4)

 def test_positive():
     assert is_perfect_square(4)
```
<br/>

Remove the `xfail` decorator and commit the fix + removal:

```diff
@@ -4,11 +4,12 @@ import pytest

 def is_perfect_square(n: int) -> bool:
     """Determine if any int i exists such that i Ã— i = n."""
+    if n < 0:
+        return False  # No negative numbers are perfect squares
+
     s = math.sqrt(n)
     return s == int(s)

-@pytest.mark.xfail(raises=ValueError, strict=True,
-                   reason="Domain error")
 def test_negative():
     """No negative numbers are perfect squares."""
     assert not is_perfect_square(-4)
```

--

One of many options to run tests against all commits:

<pre><tt class="hljs">$ git rev-list main..fix_negative_numbers | \
    xargs -I{} sh -c &apos;git checkout {} &amp;&amp; pytest -q&apos; \
    && git checkout main

HEAD is now at 04a2dd7 Fix domain error with negative numbers
<font color="#10BA13">.................                                                                                             [100%]</font>
<font color="#4BE234"><b>17 passed</b></font><font color="#10BA13"> in 0.02s</font>
Previous HEAD position was 04a2dd7 Fix domain error with negative numbers
HEAD is now at c0a9722 Add failing test for negative squares
<font color="#10BA13">..............</font><font color="#C4A000">xxx</font><font color="#10BA13">                                                                                             [100%]</font>
<font color="#4BE234"><b>14 passed</b></font>, <font color="#C4A000">3 xfailed</font><font color="#10BA13"> in 0.03s</font>
Previous HEAD position was c0a9722 Add failing test for negative squares
Switched to branch &apos;main&apos;
</tt>
</pre>

Merge and the history looks like we were using TDD the whole time:

<pre><tt class="hljs">$ git log --oneline --decorate --graph
* <font color="#C4A000">04a2dd7 (</font><font color="#34E2E2"><b>HEAD -&gt; </b></font><font color="#4BE234"><b>main</b></font><font color="#C4A000">, </font><font color="#4BE234"><b>fix_negative_numbers</b></font><font color="#C4A000">)</font> Fix domain error with negative numbers
* <font color="#C4A000">c0a9722</font> Add failing test for negative squares
* <font color="#C4A000">f9acaa7</font> Add implementation for perfect square
</tt>
</pre>

https://blog.ganssle.io/articles/2021/11/pseudo-tdd-xfail.html

--

# Advantages

- Gives you some confidence that your tests worked
- Makes it easier to verify that a test should be working
- Tells a clear story for future maintainers

<br/> <br/>

# Disadvantages

- It will be hard to get anyone else to do this.
- It's not easy to run the tests against every commit (on GitHub, Gitlab or locally)

<br/> <br/>

https://blog.ganssle.io/articles/2021/11/pseudo-tdd-xfail.html

					</textarea>
				</section>
				
				<section data-markdown data-separator="^\n---\n$" data-separator-vertical="^\n--\n$" data-notes="^Note:">
					<textarea data-template>
# Summary

- Use `xfail` for tests that you know are failing, but shouldn't be.
- Always use `skip` conditionally â€” it's for tests that aren't *supposed* to pass.
- Set `xfail_strict=True`

<br/><br/>

## Further reading

- "How and why I use pytest's `xfail`": https://blog.ganssle.io/articles/2021/11/pytest-xfail.html

- "A pseudo-TDD workflow using expected failures": https://blog.ganssle.io/articles/2021/11/pseudo-tdd-xfail.html

- "`xfail` and code coverage": https://blog.ganssle.io/articles/2021/12/xfail-coverage.html

- `pytest-runtime-xfail`: https://pypi.org/project/pytest-runtime-xfail/

- Also featured on Test & Code: https://testandcode.com/guests/paul-ganssle

					</textarea>
				</section>
				

			</div>

<div class="sbcontainer">
    <img src="images/pganssle-logos-rotated.svg"
         style="width: 1.1em;"
         class="sbitem" />
    <img src="images/ganssleio_rotated.svg"
         style="width: 1.1em;"
         class="sbitem" />
    <img src="external-images/logos/zero-black.svg"
         style="width: 1.8em;
                margin-bottom: 0.5em;"
        class="sbitem">
</div>


		</div>

		<script src="reveal.js/dist/reveal.js"></script>
		<script src="reveal.js/plugin/markdown/markdown.js"></script>
		<script src="reveal.js/plugin/zoom/zoom.js"></script>
		<script src="reveal.js/plugin/highlight/highlight.js"></script>
		
			<script src="reveal.js/plugin/notes/notes.js"></script>
		
		<script>

			// Full list of configuration options available here:
			// https://github.com/hakimel/reveal.js#configuration
			let r = Reveal();
			r.initialize({
				controls: true,
				progress: true,
				history: true,
				center: true,

				theme: r.getQueryHash().theme, // available themes are in /css/theme
				transition: 'linear', // default/cube/page/concave/zoom/linear/fade/none
                plugins: [
                    RevealMarkdown,
                    RevealHighlight,
                    RevealNotes,
                    RevealZoom,
                ],

				
					
						// Reveal options generated from mapping
						
							width: "95%",
						
							height: "95%",
						
					
				

				// The dependencies option is deprecated as of reveal.js 4.0,
				// but is kept in place for backwards compatibility reasons.
				dependencies: [
					
				]
			});

		</script>

	</body>
</html>
